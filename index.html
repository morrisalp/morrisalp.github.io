<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Morris Alper</title>

  <meta name="author" content="Morris Alper">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Morris Alper
                  </p>
                  <p>
                    I am a PhD student supervised by
                      <a href="https://www.hadarelor.com/">Dr. Hadar Averbuch-Elor</a>.
                    My research interests lie on the intersection of vision, language, and three-dimensional data, informed by how humans
                    perceive the world.
                  </p>
                  <p>
                    I began my PhD in March 2023; previously, I received my MSc with honors from Tel Aviv University in
                    Computer Science and my BSc from MIT with
                    a dual major in mathematics and linguistics. I also have experience working as a data scientist and
                    teaching
                    machine learning to industry professionals.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:morrisalper@mail.tau.ac.il">Email</a> &nbsp;/&nbsp;
                    <a href="files/CV.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=M2RsdCUAAAAJ">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://twitter.com/MorrisAlper">Twitter</a> &nbsp;/&nbsp;
		    <a href="https://bsky.app/profile/malper.bsky.social">Bluesky</a> &nbsp;/&nbsp;
                    <a href="https://github.com/morrisalp/">GitHub</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/morris-alper/">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://orcid.org/0000-0002-3533-7602">ORCID</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <!-- <a href="images/profile.png"> -->
                  <img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                    src="images/profile.png" class="hoverZoomLink">
                  <!-- </a> -->
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td class="section">
                  <h2>Publications</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr></tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/mocha.png' width="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://assafbk.github.io/mocha/">
                    <span class="papertitle">
                      Mitigating Open-Vocabulary Caption Hallucinations
                    </span>
                  </a>
                  <br>
                  <a href="https://assafbk.github.io/website/">Assaf Ben-Kish</a>,
                  <a href="https://www.linkedin.com/in/moran-yanuka-a5452a1b7/">Moran Yanuka</a>,
                  <strong>Morris Alper</strong>,
                  <a href="https://www.giryes.sites.tau.ac.il/">Raja Giryes</a>,
                  <a href="https://www.hadarelor.com/">Hadar Averbuch-Elor</a>
                  <br>
                  <em>EMNLP</em>, 2024
                  <br>
                  <p>
                    We propose a framework for addressing open-vocabulary hallucinations in image captioning models, including a new benchmark
                    and a reinforcement learning-based method to reduce such hallucinations.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/hc.png' width="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://tau-vailab.github.io/hierarcaps/">
                    <span class="papertitle">
                      Emergent Visual-Semantic Hierarchies in Image-Text Representations
                    </span>
                  </a>
                  <br>
                  <strong>Morris Alper</strong>,
                  <a href="https://www.hadarelor.com/">Hadar Averbuch-Elor</a>
                  <br>
                  <em>ECCV (Oral)</em>, 2024
                  <br>
                  <p>
                    We show that foundation VLMs like CLIP model visual-semantic hierarchies, proposing the Radial Embedding framework for probing and optimizing this knowledge and the HierarCaps dataset of ground-truth image caption hierarchies.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/ICC.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://moranyanuka.github.io/icc/">
                    <span class="papertitle">
                      ICC : Quantifying Image Caption Concreteness for Multimodal Dataset Curation
                    </span>
                  </a>
                  <br>
                  <a href="https://www.linkedin.com/in/moran-yanuka-a5452a1b7/">Moran Yanuka</a>,
                  <strong>Morris Alper</strong>,
                  <a href="https://www.hadarelor.com/">Hadar Averbuch-Elor</a>,
                  <a href="https://www.giryes.sites.tau.ac.il/">Raja Giryes</a>
                  <br>
                  <em>ACL (Findings)</em>, 2024
                  <br>
                  <p>
                    We quantify image caption concreteness using information loss in foundation vision-language models,
                    and use this score to filter web-scale multimodal datasets.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/halo.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://tau-vailab.github.io/HaLo-NeRF/">
                    <span class="papertitle">
                      HaLo-NeRF:
                      Learning Geometry-Guided Semantics for Exploring Unconstrained Photo Collections
                    </span>
                  </a>
                  <br>
                  <a href="https://www.linkedin.com/in/chen-dudai-108a72136/">Chen Dudai</a><sup>*</sup>,
                  <strong>Morris Alper</strong><sup>*</sup>,
                  <a href="https://www.linkedin.com/in/hanabezalel/">Hana Bezalel</a>,
                  <a href="https://people.cs.uchicago.edu/~ranahanocka/">Rana Hanocka</a>,
                  <a href="https://itailang.github.io/">Itai Lang</a>,
                  <a href="https://www.hadarelor.com/">Hadar Averbuch-Elor</a>
                  &nbsp;&nbsp;<small><sup>*</sup>Equal contribution</small>
                  <br>
                  <em>Eurographics</em>, 2024
                  <br>
                  <p></p>
                  <p>
                    We learn a semantic localization field for textual descriptions over collections of in-the-wild
                    images depicting a large-scale scene.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/kiki.webp' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://tau-vailab.github.io/kiki-bouba/">
                    <span class="papertitle">
                      Kiki or Bouba? Sound Symbolism in Vision-and-Language Models
                    </span>
                  </a>
                  <br>
                  <strong>Morris Alper</strong>,
                  <a href="https://www.hadarelor.com/">Hadar Averbuch-Elor</a>
                  <br>
                  <em>NeurIPS (Spotlight)</em>, 2023
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                  <a href="https://www.youtube.com/watch?v=miNsjJy7sHw">
                    <i class="fa fa-youtube-play"></i>
                    Presentation from IMVC 2024
                  </a>
                  <br>
                  <p></p>
                  <p>
                    By generating images using prompts containing pseudowords (nonsense words) and analyzing their
                    shapes,
                    we show that AI image generation models show sound-shape associations similar to those known from
                    human psychology.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/hhi.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://tau-vailab.github.io/learning-interactions/">
                    <span class="papertitle">
                      Learning Human-Human Interactions in Images from Weak Textual Supervision
                    </span>
                  </a>
                  <br>
                  <strong>Morris Alper</strong>,
                  <a href="https://www.hadarelor.com/">Hadar Averbuch-Elor</a>
                  <br>
                  <em>ICCV</em>, 2023
                  <br>
                  <p></p>
                  <p>
                    We model human-human interaction understanding in images as free text
                    generation, provide a new benchmark and show how to learn this with weak supervision from Internet
                    image captions.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/ibb.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://tau-vailab.github.io/isbertblind/">
                    <span class="papertitle">
                      Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language
                      Understanding
                    </span>
                  </a>
                  <br>
                  <strong>Morris Alper</strong><sup>*</sup>,
                  <a href="https://www.linkedin.com/in/michael-fiman/">Michael Fiman</a><sup>*</sup>,
                  <a href="https://www.hadarelor.com/">Hadar Averbuch-Elor</a>
                  &nbsp;&nbsp;<small><sup>*</sup>Equal contribution</small>
                  <br>
                  <em>CVPR</em>, 2023
                  <br>
                  <p></p>
                  <p>
                    We find that multimodally trained text encoders outperform unimodally
                    trained text encoders on visual reasoning in text.
                  </p>
                </td>
              </tr>



            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td class="section">
                  <h2>Miscellanea</h2>
                </td>
              </tr>
              <tr>
                <td>
                  <b>Featured in the News:</b>
                  <ul>
                    <li>Our work on the Kiki-Bouba effect and sound symbolism in AI models was featured
                      <a
                        href="https://www.haaretz.co.il/science/2024-01-25/ty-article-magazine/.premium/0000018d-3b2d-d32b-adcf-ff6f828f0000">
                        in the Haaretz newspaper</a> (Hebrew language). I was also interviewed about this work on the
                      Kan Tarbut morning radio program
                      (<a href="https://www.kan.org.il/content/kan/podcasts/p-8270/699060/">recording link</a>,
                      46:30-60:00, Hebrew language).
                    </li>
                  </ul>
                </td>
              </tr>
              <tr>
                <td>
                  <b>Blog Posts and Projects:</b>
                  <ul>
                    <li><a
                        href="https://medium.com/towards-data-science/unikud-adding-vowels-to-hebrew-text-with-deep-learning-powered-by-dagshub-56d238e22d3f">
                        UNIKUD: Adding Vowels to Hebrew Text with Deep Learning
                      </a></li>
                    <li><a
                        href="https://towardsdatascience.com/taatiknet-sequence-to-sequence-learning-for-hebrew-transliteration-4c9175a90c23">
                        TaatikNet: Sequence-to-Sequence Learning for Hebrew Transliteration
                      </a></li>
                  </ul>
                </td>
              </tr>
              <tr>
                <td>
                  <b>Awards and Honors:</b>
                  <ul>
                    <li>
                      As a high school student, I qualified to join the USA team at the <a
                        href="https://ioling.org/index.html">International
                        Linguistics Olympiad</a> (IOL) for three years (2008, 2009, 2011). In particular, in 2011 I won
                      1st place (gold medal)
                      on individual and team rounds in 2011 as well as receiving the Zhurinski Memorial Prize and a best
                      solution award. I also received a silver medal (tied for 6th place) in 2008 and our team won the
                      team round in 2009.
                    </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p class="footer">
                    Page format adapted from <a href="https://github.com/jonbarron/website">Jon Barron's template</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
